# OpenFlaminKO <ğŸ¦©ğŸ‡°ğŸ‡·>
![OpenFlaminKO](https://github.com/Marker-Inc-Korea/OpenFlaminKO/assets/98331298/f45b07b7-f0bb-4a02-acff-19e20e933cfd)  
- [Polyglot-Ko](https://huggingface.co/EleutherAI/polyglot-ko-1.3b)ì„ í™œìš©í•œ image-text multimodal  
  
# Introduction
![Openflamingo ì„±ëŠ¥](https://github.com/Marker-Inc-Korea/OpenFlaminKO/assets/98331298/c27c8c32-e0a5-432d-b29e-c79d09e4b1b2)
- ìµœê·¼ LLMì—´í’ì´ ë¶ˆê³  ìˆëŠ” ê°€ìš´ë°, í•œêµ­ì–´ LLMì¸ Polyglot-KOì˜ ë“±ì¥ìœ¼ë¡œ êµ­ë‚´ì—ì„œë„ ChatGPTì™€ ìœ ì‚¬í•œ LLMì„ ë§Œë“¤ê³ ìí•˜ëŠ” ì‹œë„ì™€ ë…¸ë ¥ë“¤ì´ ì—„ì²­ í™œë°œí•˜ê²Œ ì´ë£¨ì–´ì§€ê³  ìˆìŒ.
- Polyglot-KOë¥¼ LORA ë°©ì‹ì„ í™œìš©í•˜ì—¬ ì—¬ëŸ¬ NLP ë¶„ì•¼ì—ì„œ ì ‘ëª©í•˜ê³  ìˆì§€ë§Œ, multimodalë¡œ ì´ìš©ë˜ëŠ” ì‚¬ë¡€ê°€ ì•„ì§ ë‚˜ì˜¤ì§€ ì•ŠìŒ.
- MultiModalì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ì—„ì²­ë‚œ ì–‘ì˜ ë°ì´í„°ì…‹ì´ í•„ìš”í•˜ì§€ë§Œ, í˜„ì¬ ì¡´ì¬í•˜ëŠ” multimodal dataset(i.e., [LAION](https://laion.ai/), [MMC4](https://github.com/allenai/mmc4))ë“¤ì€ ëŒ€ë¶€ë¶„ ì˜ì–´ captionìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , í•œêµ­ì–´ì˜ ë¹„ì¤‘ì´ ê±°ì˜ ì—†ê¸° ë•Œë¬¸ì— í•œêµ­ì–´ Multimodalì„ ë§Œë“œëŠ” ê²ƒì„ ì‚¬ì‹¤ìƒ ì‰½ì§€ ì•Šì€ ë„ì „ì„.
- ê¸°ì¡´ì˜ LLMì„ í™œìš©í•˜ì—¬ multimodalë¡œ ë§Œë“  [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)ë¡œ ë¶€í„° ì˜ê°ì„ ë°›ì•„ì„œ, Polyglot-KOë¥¼ multimodalë¡œ ì„¤ê³„í•˜ê³ ì í•˜ëŠ” ë§ˆìŒì´ ìƒê²¨ì„œ OpenFlminKO modelë¥¼ ë§Œë“¤ê¸° ì‹œì‘í•¨.
- ê¸°ì¡´ SOTA multimodalë“¤ì€ í•œêµ­ì–´ì— ëŒ€í•œ ì„±ëŠ¥ì´ ë§¤ìš° ì•ˆ ì¢‹ì•˜ê¸° ë•Œë¬¸ì—, í•œêµ­ì–´ multimodalë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ì²«ê±¸ìŒì´ë¼ëŠ” ê²ƒì— ì˜ì˜ë¥¼ ë‘”ë‹¤.
- ë³¸ ì—°êµ¬ëŠ” (ì£¼)ë§ˆì»¤ì™€ (ì£¼)ë¯¸ë””ì–´ê·¸ë£¹ì‚¬ëŒê³¼ìˆ²ì˜ ì˜¤í”ˆì†ŒìŠ¤ LLM ì—°êµ¬ ì»¨ì†Œì‹œì—„ì—ì„œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.

# Dependencies
- [ChromeDriver](https://chromedriver.chromium.org/downloads) ì„¤ì¹˜
- [7-zip](https://www.7-zip.org/) í™˜ê²½ë³€ìˆ˜ ë“±ë¡
- Pytorch >= 2.0 ê¶Œì¥
  
# KO-LAION Dataset
```
translation
â”œâ”€â”€ sample                    
â”‚   â”œâ”€â”€ complete
|   â”œâ”€â”€ shard_00001.tar # ë²ˆì—­í•  tar íŒŒì¼
â”‚   â”œâ”€â”€ shard_00002.tar
â”‚   â””â”€â”€ ...
â”œâ”€â”€ chromedriver.exe
â”œâ”€â”€ txt_translation.py
â”œâ”€â”€ set_translation.py
â”œâ”€â”€ tar_txt_preprocessing.py
â””â”€â”€ ...
```
> ë°ì´í„° ë²ˆì—­ì„ ìœ„í•œ íŒŒì¼ ê²½ë¡œëŠ” ìœ„ì™€ ê°™ë‹¤.
  
- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)ì—ì„œ í™œìš©í–ˆë˜ LAION datasetì„ ê¸°ë°˜ìœ¼ë¡œ [DeepL](https://www.deepl.com/translator)ì„ í†µí•´ì„œ ë²ˆì—­ì„ ì‹œë„í•˜ì—¬ 400ë§Œê°œ captionì„ ë²ˆì—­í•¨.  
- [Webdataset](https://github.com/webdataset/webdataset) í˜•ì‹ì„ ì§€ì›í•˜ê¸° ë•Œë¬¸ì— LAION ë°ì´í„°ì…‹ì„ taríŒŒì¼ë¡œ ì´ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ê´€ë ¨ ì½”ë“œëŠ” [LAION_TRAIN](https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/train)ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”.  
- HuggingFaceì— [KO-LAION-SUBSET](Soon...)ì„ í†µí•´ì„œ ë‹¤ìš´ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (Not update...)  
```
txt_translation.py # taríŒŒì¼ì˜ ì••ì¶•ì„ í’€ê³ , txtíŒŒì¼ì„ ë²ˆì—­í•˜ëŠ” ê³¼ì •
set_translation.py # ë²ˆì—­ì´ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , taríŒŒì¼ë¡œ ë‹¤ì‹œ ì••ì¶•í•˜ëŠ” ê³¼ì •
tar_txt_preprocessing.py # text ì „ì²˜ë¦¬ ì½”ë“œ
```
> ìœ„ì˜ ì½”ë“œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ì„œ LAION datasetì„ ë²ˆì—­í•©ë‹ˆë‹¤.  
> ë°ì´í„° ì „ì²˜ë¦¬: 1) ì´ëª¨ì§€ ì œê±° 2) í•œêµ­ì–´ê°€ ì—†ëŠ” caption ì œê±° 3) empty sentence ì œê±°  
  
```
open_flamingo
â”œâ”€â”€ laion_data                    
â”‚   â”œâ”€â”€ shard_00000.tar
|   â”œâ”€â”€ shard_00001.tar       
â”‚   â”œâ”€â”€ shard_00002.tar
â”‚   â””â”€â”€ ...
â”œâ”€â”€ open_flamingo
â”œâ”€â”€ OpenKyujinpie_v1
â””â”€â”€ ...
```
> ë°ì´í„° ê²½ë¡œëŠ” ìœ„ì™€ ê°™ì´ ì„¤ì •í•©ë‹ˆë‹¤.  

# Training Code
- [COLAB](https://colab.research.google.com/drive/1_TEJopoN7a4jeDOQZ0Dse4fIEXUFtYRC#scrollTo=l_YCMEVRj3rp) ì½”ë“œë¥¼ í†µí•´ì„œ ì‹¤í–‰í•´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> ë‹¨, A100 GPUì—ì„œë§Œ ì‹¤í–‰ì´ ë˜ê¸° ë•Œë¬¸ì—, ë¹„ìš©ì´ ë§ì´ ë°œìƒí•©ë‹ˆë‹¤.  
> Polyglot-KO-1.3bì—ì„œ ì‹¤í–‰ì´ ë©ë‹ˆë‹¤. 5.8b ì´ìƒì€ ì›í•˜ì‹¤ ê²½ìš°, ë” ë§ì€ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤.  
  
# Evaluation Code
- ëª¨ë¸ì˜ checkpointëŠ” ì•„ë˜ì—ì„œ ë‹¤ìš´ë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (Not update...)  
    
| Model | Dataset | Link |  
| ------------- | ------------- | ------------- |  
| `OpenFlaminKO-400K` | [KO-LAION-400K](Not) | NaN |  
| `OpenFlaminKO-1M` | [KO-LAION-1M](Not) | [Checkpoint](Not) |  
| `OpenFlaminKO-4M` | [KO-LAION-4M](Not) | [Checkpoint](Not) |  
   
```python
# 50 epoch model
from open_flamingo import create_model_and_transforms

# Default setting
model, image_processor, tokenizer = create_model_and_transforms(
    clip_vision_encoder_path="ViT-L-14",
    clip_vision_encoder_pretrained="openai",
    lang_encoder_path="EleutherAI/polyglot-ko-1.3b", # PolyGlot 1.3B
    tokenizer_path="EleutherAI/polyglot-ko-1.3b", # PolyGlot 1.3B
    cross_attn_every_n_layers=1
)

# load checkpoint
checkpoint_path = (Coming soon...)
print("Model path:", checkpoint_path)
model.load_state_dict(torch.load(checkpoint_path), strict=False)
model.eval() # Freeze
```
>Model implementation
  
```python
# Image captioning by github example
from PIL import Image
import requests
import torch

"""
Step 1: Load images
"""
demo_image_two = Image.open(
    requests.get(
        "http://images.cocodataset.org/test-stuff2017/000000028137.jpg",
        stream=True
    ).raw
)

query_image = Image.open(
    requests.get(
        "http://images.cocodataset.org/val2017/000000039769.jpg",
        stream=True
    ).raw
)

"""
Step 2: Preprocessing images
Details: For OpenFlamingo, we expect the image to be a torch tensor of shape
 batch_size x num_media x num_frames x channels x height x width.
 In this case batch_size = 1, num_media = 3, num_frames = 1,
 channels = 3, height = 224, width = 224.
"""
vision_x = [image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]
vision_x = torch.cat(vision_x, dim=0)
vision_x = vision_x.unsqueeze(1).unsqueeze(0)

"""
Step 3: Preprocessing text
Details: In the text we expect an <image> special token to indicate where an image is.
 We also expect an <|endofchunk|> special token to indicate the end of the text
 portion associated with an image.
 +) Translation by DeepL.
"""
tokenizer.padding_side = "left" # For generation padding tokens should be on the left
lang_x = tokenizer(
    ["<image>í•´ë‹¹ ì´ë¯¸ì§€ëŠ” ìš•ì‹¤ ì„¸ë©´ëŒ€ì…ë‹ˆë‹¤.<|endofchunk|><image>í•´ë‹¹ ì´ë¯¸ì§€ëŠ”"], # new version
    return_tensors="pt",
)


"""
Step 4: Generate text
"""
generated_text = model.generate(
    vision_x=vision_x,
    lang_x=lang_x["input_ids"],
    attention_mask=lang_x["attention_mask"],
    max_new_tokens=20,
    num_beams=3, #
)

print("Generated text: ", tokenizer.decode(generated_text[0]))
```
>Simple test code  
  
# Performance (Not update...)  
- KO-VQAv2 (VQA accuracy)  
  
| Model | Dataset | 0-shot |  
| ------------- | ------------- | ------------- |  
| `OpenFlaminKO-400K` | [KO-LAION-400K](Not) | NaN |  
| `OpenFlaminKO-1M` | [KO-LAION-1M](Not) | NaN |  
| `OpenFlaminKO-4M` | [KO-LAION-4M](Not) | NaN |  

> Performance ì¸¡ì •ì„ ìœ„í•´ [VQAv2](https://visualqa.org/download.html)ë¥¼ DeepLì„ í™œìš©í•˜ì—¬ ë²ˆì—­í•¨.
  
# Acknowledgement
[OpenFlamingo](https://github.com/mlfoundations/open_flamingo)  
[Polyglot-KO](https://huggingface.co/EleutherAI/polyglot-ko-1.3b)  
[CLIP-VIT](https://huggingface.co/openai/clip-vit-large-patch14)  



