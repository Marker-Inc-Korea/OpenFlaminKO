# OpenFlaminKO <ü¶©üá∞üá∑>
![OpenFlaminKO](https://github.com/Marker-Inc-Korea/OpenFlaminKO/assets/98331298/f45b07b7-f0bb-4a02-acff-19e20e933cfd)  
- [Polyglot-Ko](https://huggingface.co/EleutherAI/polyglot-ko-1.3b)ÏùÑ ÌôúÏö©Ìïú image-text multimodal  
  
# Introduction
![Openflamingo ÏÑ±Îä•](https://github.com/Marker-Inc-Korea/OpenFlaminKO/assets/98331298/c27c8c32-e0a5-432d-b29e-c79d09e4b1b2)
- ÏµúÍ∑º LLMÏó¥ÌíçÏù¥ Î∂àÍ≥† ÏûàÎäî Í∞ÄÏö¥Îç∞, ÌïúÍµ≠Ïñ¥ LLMÏù∏ Polyglot-KOÏùò Îì±Ïû•ÏúºÎ°ú Íµ≠ÎÇ¥ÏóêÏÑúÎèÑ ChatGPTÏôÄ Ïú†ÏÇ¨Ìïú LLMÏùÑ ÎßåÎì§Í≥†ÏûêÌïòÎäî ÏãúÎèÑÏôÄ ÎÖ∏Î†•Îì§Ïù¥ ÏóÑÏ≤≠ ÌôúÎ∞úÌïòÍ≤å Ïù¥Î£®Ïñ¥ÏßÄÍ≥† ÏûàÏùå.
- Polyglot-KOÎ•º LORA Î∞©ÏãùÏùÑ ÌôúÏö©ÌïòÏó¨ Ïó¨Îü¨ NLP Î∂ÑÏïºÏóêÏÑú Ï†ëÎ™©ÌïòÍ≥† ÏûàÏßÄÎßå, multimodalÎ°ú Ïù¥Ïö©ÎêòÎäî ÏÇ¨Î°ÄÍ∞Ä ÏïÑÏßÅ ÎÇòÏò§ÏßÄ ÏïäÏùå.
- MultiModalÏùÑ ÎßåÎì§Í∏∞ ÏúÑÌï¥ÏÑúÎäî ÏóÑÏ≤≠ÎÇú ÏñëÏùò Îç∞Ïù¥ÌÑ∞ÏÖãÏù¥ ÌïÑÏöîÌïòÏßÄÎßå, ÌòÑÏû¨ Ï°¥Ïû¨ÌïòÎäî multimodal dataset(i.e., [LAION](https://laion.ai/), [MMC4](https://github.com/allenai/mmc4))Îì§ÏùÄ ÎåÄÎ∂ÄÎ∂Ñ ÏòÅÏñ¥ captionÏúºÎ°ú Ïù¥Î£®Ïñ¥Ï†∏ ÏûàÍ≥†, ÌïúÍµ≠Ïñ¥Ïùò ÎπÑÏ§ëÏù¥ Í±∞Ïùò ÏóÜÍ∏∞ ÎïåÎ¨∏Ïóê ÌïúÍµ≠Ïñ¥ MultimodalÏùÑ ÎßåÎìúÎäî Í≤ÉÏùÑ ÏÇ¨Ïã§ÏÉÅ ÏâΩÏßÄ ÏïäÏùÄ ÎèÑÏ†ÑÏûÑ.
- Í∏∞Ï°¥Ïùò LLMÏùÑ ÌôúÏö©ÌïòÏó¨ multimodalÎ°ú ÎßåÎì† [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)Î°ú Î∂ÄÌÑ∞ ÏòÅÍ∞êÏùÑ Î∞õÏïÑÏÑú, Polyglot-KOÎ•º multimodalÎ°ú ÏÑ§Í≥ÑÌïòÍ≥†Ïûê ÌïòÎäî ÎßàÏùåÏù¥ ÏÉùÍ≤®ÏÑú OpenFlminKO modelÎ•º ÎßåÎì§Í∏∞ ÏãúÏûëÌï®.
- Í∏∞Ï°¥ SOTA multimodalÎì§ÏùÄ ÌïúÍµ≠Ïñ¥Ïóê ÎåÄÌïú ÏÑ±Îä•Ïù¥ Îß§Ïö∞ Ïïà Ï¢ãÏïòÍ∏∞ ÎïåÎ¨∏Ïóê, ÌïúÍµ≠Ïñ¥ multimodalÎ•º ÎßåÎì§Í∏∞ ÏúÑÌïú Ï≤´Í±∏ÏùåÏù¥ÎùºÎäî Í≤ÉÏóê ÏùòÏùòÎ•º ÎëîÎã§.
- Î≥∏ Ïó∞Íµ¨Îäî (Ï£º)ÎßàÏª§ÏôÄ (Ï£º)ÎØ∏ÎîîÏñ¥Í∑∏Î£πÏÇ¨ÎûåÍ≥ºÏà≤Ïùò Ïò§ÌîàÏÜåÏä§ LLM Ïó∞Íµ¨ Ïª®ÏÜåÏãúÏóÑÏóêÏÑú ÏßÑÌñâÎêòÏóàÏäµÎãàÎã§.  
  
# KO-LAION Dataset
- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)ÏóêÏÑú ÌôúÏö©ÌñàÎçò LAION datasetÏùÑ Í∏∞Î∞òÏúºÎ°ú DeepLÏùÑ ÌÜµÌï¥ÏÑú Î≤àÏó≠ÏùÑ ÏãúÎèÑÌïòÏó¨ 400ÎßåÍ∞ú captionÏùÑ Î≤àÏó≠Ìï®.  
- [Webdataset](https://github.com/webdataset/webdataset) ÌòïÏãùÏùÑ ÏßÄÏõêÌïòÍ∏∞ ÎïåÎ¨∏Ïóê LAION Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ tarÌååÏùºÎ°ú Ïù¥Ïö©Ìï¥Ïïº Ìï©ÎãàÎã§. Í¥ÄÎ†® ÏΩîÎìúÎäî [LAION_TRAIN](https://github.com/mlfoundations/open_flamingo/tree/main/open_flamingo/train)Î•º Ï∞∏Í≥†Ìï¥Ï£ºÏÑ∏Ïöî.  
- HuggingFaceÏóê [KO-LAION-SUBSET](Soon...)ÏùÑ ÌÜµÌï¥ÏÑú Îã§Ïö¥ Î∞õÏúºÏã§ Ïàò ÏûàÏäµÎãàÎã§.  
```
(Code coming soon...)
```
> ÏúÑÏùò ÏΩîÎìúÎ•º ÏàúÏ∞®Ï†ÅÏúºÎ°ú Ïã§ÌñâÌïòÏó¨ÏÑú LAION datasetÏùÑ Î≤àÏó≠Ìï©ÎãàÎã§.

```
open_flamingo
‚îú‚îÄ‚îÄ laion_data                    
‚îÇ   ‚îú‚îÄ‚îÄ shard_00000.tar
|   ‚îú‚îÄ‚îÄ shard_00001.tar       
‚îÇ   ‚îú‚îÄ‚îÄ shard_00002.tar
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ open_flamingo
‚îú‚îÄ‚îÄ OpenKyujinpie_v1
‚îî‚îÄ‚îÄ ...
```
> Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°úÎäî ÏúÑÏôÄ Í∞ôÏù¥ ÏÑ§Ï†ïÌï©ÎãàÎã§.  

# Training Code
- [COLAB](https://colab.research.google.com/drive/1_TEJopoN7a4jeDOQZ0Dse4fIEXUFtYRC#scrollTo=l_YCMEVRj3rp) ÏΩîÎìúÎ•º ÌÜµÌï¥ÏÑú Ïã§ÌñâÌï¥Î≥¥Ïã§ Ïàò ÏûàÏäµÎãàÎã§.
> Îã®, A100 GPUÏóêÏÑúÎßå Ïã§ÌñâÏù¥ ÎêòÍ∏∞ ÎïåÎ¨∏Ïóê, ÎπÑÏö©Ïù¥ ÎßéÏù¥ Î∞úÏÉùÌï©ÎãàÎã§.  
> Polyglot-KO-1.3bÏóêÏÑú Ïã§ÌñâÏù¥ Îê©ÎãàÎã§. 5.8b Ïù¥ÏÉÅÏùÄ ÏõêÌïòÏã§ Í≤ΩÏö∞, Îçî ÎßéÏùÄ Ïª¥Ìì®ÌåÖ ÏûêÏõêÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.  
  
# Evaluation Code
- Î™®Îç∏Ïùò checkpointÎäî ÏïÑÎûòÏóêÏÑú Îã§Ïö¥Î∞õÏúºÏã§ Ïàò ÏûàÏäµÎãàÎã§.
    
| Model | Dataset | Link |  
| ------------- | ------------- | ------------- |  
| `OpenFlaminKO-400K` | [KO-LAION-400K](Not) | [Checkpoint](Not) |  
| `OpenFlaminKO-1M` | [KO-LAION-1M](Not) | [Checkpoint](Not) |  
| `OpenFlaminKO-4M` | [KO-LAION-4M](Not) | [Checkpoint](Not) |  
   
```python
# 50 epoch model
from open_flamingo import create_model_and_transforms

# Default setting
model, image_processor, tokenizer = create_model_and_transforms(
    clip_vision_encoder_path="ViT-L-14",
    clip_vision_encoder_pretrained="openai",
    lang_encoder_path="EleutherAI/polyglot-ko-1.3b", # PolyGlot 1.3B
    tokenizer_path="EleutherAI/polyglot-ko-1.3b", # PolyGlot 1.3B
    cross_attn_every_n_layers=1
)

# load checkpoint
checkpoint_path = (Coming soon...)
print("Model path:", checkpoint_path)
model.load_state_dict(torch.load(checkpoint_path), strict=False)
model.eval() # Freeze
```
>Model implementation
  
```python
# Image captioning by github example
from PIL import Image
import requests
import torch

"""
Step 1: Load images
"""
demo_image_two = Image.open(
    requests.get(
        "http://images.cocodataset.org/test-stuff2017/000000028137.jpg",
        stream=True
    ).raw
)

query_image = Image.open(
    requests.get(
        "http://images.cocodataset.org/val2017/000000039769.jpg",
        stream=True
    ).raw
)

"""
Step 2: Preprocessing images
Details: For OpenFlamingo, we expect the image to be a torch tensor of shape
 batch_size x num_media x num_frames x channels x height x width.
 In this case batch_size = 1, num_media = 3, num_frames = 1,
 channels = 3, height = 224, width = 224.
"""
vision_x = [image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]
vision_x = torch.cat(vision_x, dim=0)
vision_x = vision_x.unsqueeze(1).unsqueeze(0)

"""
Step 3: Preprocessing text
Details: In the text we expect an <image> special token to indicate where an image is.
 We also expect an <|endofchunk|> special token to indicate the end of the text
 portion associated with an image.
 +) Translation by DeepL.
"""
tokenizer.padding_side = "left" # For generation padding tokens should be on the left
lang_x = tokenizer(
    ["<image>Ìï¥Îãπ Ïù¥ÎØ∏ÏßÄÎäî ÏöïÏã§ ÏÑ∏Î©¥ÎåÄÏûÖÎãàÎã§.<|endofchunk|><image>Ìï¥Îãπ Ïù¥ÎØ∏ÏßÄÎäî"], # new version
    return_tensors="pt",
)


"""
Step 4: Generate text
"""
generated_text = model.generate(
    vision_x=vision_x,
    lang_x=lang_x["input_ids"],
    attention_mask=lang_x["attention_mask"],
    max_new_tokens=20,
    num_beams=3, #
)

print("Generated text: ", tokenizer.decode(generated_text[0]))
```
>Simple test code  
  
# Performance

- VQAv2 (VQA accuracy)
  
| Model | Dataset | 0-shot |  
| ------------- | ------------- | ------------- |  
| `OpenFlaminKO-400K` | [KO-LAION-400K](Not) | NaN |  
| `OpenFlaminKO-1M` | [KO-LAION-1M](Not) | NaN |  
| `OpenFlaminKO-4M` | [KO-LAION-4M](Not) | NaN |  
  
# Acknowledgement
[OpenFlamingo](https://github.com/mlfoundations/open_flamingo)  
[Polyglot-KO](https://huggingface.co/EleutherAI/polyglot-ko-1.3b)  
[CLIP-VIT](https://huggingface.co/openai/clip-vit-large-patch14)  



